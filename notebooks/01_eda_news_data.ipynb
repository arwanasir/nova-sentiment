{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27799c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ee1b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "229d082a",
   "metadata": {},
   "source": [
    "#loading the data and basic infos about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5624ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw_analyst_ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77aa5f",
   "metadata": {},
   "source": [
    "#shapes of the data, overvies , missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0140971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84115fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d736fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" DATASET OVERVIEW\")\n",
    "print(f\"Total records: {df.shape[0]:,}\")\n",
    "print(f\"Total columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COLUMN NAMES\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA TYPES & MISSING VALUES\")\n",
    "info_df = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Missing %': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Unique Values': df.nunique()\n",
    "})\n",
    "display(info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b59397",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BASIC STATISTICS\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(\"SAMPLE HEADLINES\")\n",
    "for i, headline in enumerate(df['headline'].head(5)):\n",
    "    print(f\"{i+1}. {headline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA QUALITY CHECKS\")\n",
    "\n",
    "\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "\n",
    "empty_headlines = df['headline'].isna().sum()\n",
    "print(f\"Empty headlines: {empty_headlines}\")\n",
    "\n",
    "\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    invalid_dates = df['date'].isna().sum()\n",
    "    print(f\"Invalid dates: {invalid_dates}\")\n",
    "    \n",
    "    if invalid_dates == 0:\n",
    "        date_range = df['date'].agg(['min', 'max'])\n",
    "        print(f\"Date range: {date_range['min']} to {date_range['max']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840203bd",
   "metadata": {},
   "source": [
    "Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62537ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#headline length \n",
    "df['headline_length'] = df['headline'].str.len()\n",
    "df['word_count'] = df['headline'].str.split().str.len()\n",
    "\n",
    "print(\"Headline Length Statistics:\")\n",
    "print(df['headline_length'].describe())\n",
    "\n",
    "print(\"Word Count Statistics:\")\n",
    "print(df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2762c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#publisher analysis\n",
    "#counting articles per publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "print(f\"total unique publisher: {len(publisher_counts)}\")\n",
    "\n",
    "print(\"\\nTop 15 Publishers by Article Count:\")\n",
    "top_publishers = publisher_counts.head(15)\n",
    "for i, (publisher, count) in enumerate(top_publishers.items(), 1):\n",
    "    print(f\"{i:2d}. {publisher}: {count:>4} articles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#publication date trend analysis\n",
    "df['datetime'] = pd.to_datetime(df['date'])\n",
    "df['date_only'] = df['datetime'].dt.date\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['week'] = df['datetime'].dt.isocalendar().week\n",
    "\n",
    "print(\"Date Range Analysis:\")\n",
    "print(f\"Earliest publication: {df['datetime'].min()}\")\n",
    "print(f\"Latest publication: {df['datetime'].max()}\")\n",
    "print(f\"Total time span: {df['datetime'].max() - df['datetime'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headline length distribution\n",
    "plt.hist(df['headline_length'], bins=30)\n",
    "plt.title('Headline Length Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Top publishers bar chart\n",
    "top_publishers.head(10).plot(kind='bar')\n",
    "plt.title('Top 10 Publishers by Article Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ce27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" DAILY PUBLICATION FREQUENCY\")\n",
    "# Count articles per day\n",
    "daily_counts = df['date_only'].value_counts().sort_index()\n",
    "print(f\"Total days with publications: {len(daily_counts)}\")\n",
    "print(f\"Average articles per day: {daily_counts.mean():.1f}\")\n",
    "print(f\"Busiest day: {daily_counts.idxmax()} with {daily_counts.max()} articles\")\n",
    "print(f\"Quietest day: {daily_counts.idxmin()} with {daily_counts.min()} articles\")\n",
    "\n",
    "\n",
    "mean_daily = daily_counts.mean()\n",
    "std_daily = daily_counts.std()\n",
    "spike_threshold = mean_daily + std_daily\n",
    "spike_days = daily_counts[daily_counts > spike_threshold]\n",
    "\n",
    "print(f\"\\n Publication Spikes (>{spike_threshold:.1f} articles):\")\n",
    "print(f\"Found {len(spike_days)} days with unusually high publication volume\")\n",
    "for date, count in spike_days.head(10).items():\n",
    "    print(f\"  {date}: {count} articles\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c76082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication trends visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Daily article count over time\n",
    "ax1.plot(daily_counts.index, daily_counts.values, color='blue', alpha=0.7, linewidth=1)\n",
    "ax1.axhline(y=mean_daily, color='red', linestyle='--', label=f'Average: {mean_daily:.1f}')\n",
    "ax1.axhline(y=spike_threshold, color='orange', linestyle='--', label=f'Spike Threshold: {spike_threshold:.1f}')\n",
    "\n",
    "# Highlight spike days\n",
    "for date, count in spike_days.items():\n",
    "    ax1.plot(date, count, 'ro', markersize=4)\n",
    "\n",
    "ax1.set_title('Daily Article Publication Frequency Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Articles')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Day of week analysis\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "articles_by_weekday = df['day_of_week'].value_counts().reindex(day_order)\n",
    "\n",
    "ax2.bar(articles_by_weekday.index, articles_by_weekday.values, color='green', alpha=0.7)\n",
    "ax2.set_title('Article Publication by Day of Week', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Articles')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(articles_by_weekday.values):\n",
    "    ax2.text(i, v + 5, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Insight: Busiest weekday: {articles_by_weekday.idxmax()} with {articles_by_weekday.max()} articles\")\n",
    "print(f\"Insight: {len(spike_days)} days had unusually high publication volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HOURLY & MONTHLY PATTERNS \")\n",
    "\n",
    "# Hourly distribution\n",
    "hourly_counts = df['hour'].value_counts().sort_index()\n",
    "peak_hour = hourly_counts.idxmax()\n",
    "\n",
    "# Monthly distribution  \n",
    "monthly_counts = df['month'].value_counts().sort_index()\n",
    "peak_month = monthly_counts.idxmax()\n",
    "\n",
    "print(f\"Peak publication hour: {peak_hour}:00 with {hourly_counts.max()} articles\")\n",
    "print(f\"Peak publication month: Month {peak_month} with {monthly_counts.max()} articles\")\n",
    "\n",
    "# Create subplots for hourly/monthly patterns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Hourly distribution\n",
    "ax1.bar(hourly_counts.index, hourly_counts.values, color='purple', alpha=0.7)\n",
    "ax1.set_xlabel('Hour of Day (24h)')\n",
    "ax1.set_ylabel('Number of Articles')\n",
    "ax1.set_title('Article Publication by Hour of Day')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly distribution\n",
    "ax2.bar(monthly_counts.index, monthly_counts.values, color='brown', alpha=0.7)\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('Number of Articles')\n",
    "ax2.set_title('Article Publication by Month')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ce027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Counter\n",
    "\n",
    "\n",
    "print(\"EVENT-DRIVEN PUBLICATION ANALYSIS\")\n",
    "\n",
    "\n",
    "print(\"Investigating spike days for potential market events...\")\n",
    "\n",
    "for date, count in spike_days.head(5).items():\n",
    "    day_articles = df[df['date_only'] == date]\n",
    "    print(f\"\\n {date} - {count} articles (Spike Day):\")\n",
    "    \n",
    "   \n",
    "    day_headlines = ' '.join(day_articles['headline'].astype(str))\n",
    "    words = day_headlines.lower().split()\n",
    "    common_words = Counter(words).most_common(8)\n",
    "    \n",
    "    print(f\"   Top keywords: {[word for word, freq in common_words if len(word) > 3]}\")\n",
    "    print(f\"   Sample headlines:\")\n",
    "    for headline in day_articles['headline'].head(2):\n",
    "        print(f\"     - {headline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9e82f",
   "metadata": {},
   "source": [
    "Text Analysis(Topic Modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8113f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "financial_stopwords = {'said', 'inc', 'corp', 'ltd', 'co', 'stock', 'stocks'}\n",
    "stop_words.update(financial_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a91e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MOST COMMON WORDS ANALYSIS\")\n",
    "\n",
    "# Combine all headlines into one text\n",
    "all_headlines = ' '.join(df['headline'].astype(str))\n",
    "\n",
    "# Clean and tokenize\n",
    "words = re.findall(r'\\b[a-zA-Z]{3,}\\b', all_headlines.lower())  # Words with 3+ letters\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Get most common words\n",
    "word_freq = Counter(filtered_words)\n",
    "common_words = word_freq.most_common(20)\n",
    "\n",
    "print(\"Top 20 Most Common Words:\")\n",
    "for i, (word, count) in enumerate(common_words, 1):\n",
    "    print(f\"{i:2d}. {word:15} : {count:>4} occurrences\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "words, counts = zip(*common_words)\n",
    "plt.barh(words, counts, color='teal', alpha=0.7)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Most Common Words in Financial Headlines')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SPECIFIC KEYWORD ANALYSIS\")\n",
    "\n",
    "\n",
    "target_keywords = [\n",
    "    'FDA', 'approval', 'price target', 'earnings', \n",
    "    'stock', 'high', 'low', 'profit', 'loss',\n",
    "    'revenue', 'growth', 'dividend', 'merge',\n",
    "    'acquisition', 'buy', 'sell', 'upgrade', 'downgrade'\n",
    "]\n",
    "\n",
    "print(\"Keyword Frequency Analysis:\")\n",
    "keyword_results = {}\n",
    "\n",
    "for keyword in target_keywords:\n",
    "    count = df[df['headline'].str.contains(keyword, case=False, na=False)].shape[0]\n",
    "    keyword_results[keyword] = count\n",
    "    print(f\"'{keyword}': {count:>3} articles\")\n",
    "\n",
    "\n",
    "print(f\"\\nMost frequent specific keywords:\")\n",
    "sorted_keywords = sorted(keyword_results.items(), key=lambda x: x[1], reverse=True)\n",
    "for keyword, count in sorted_keywords[:10]:\n",
    "    if count > 0:\n",
    "        print(f\"   {keyword:15} : {count:>3} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization for top keywords\n",
    "top_keywords = dict(sorted_keywords[:12])\n",
    "if top_keywords:  # Only plot if we have data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(list(top_keywords.keys()), list(top_keywords.values()), color='coral', alpha=0.7)\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.title('Top Financial Keywords in News Headlines')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (keyword, count) in enumerate(top_keywords.items()):\n",
    "        plt.text(count + 0.5, i, str(count), va='center')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2b9e0",
   "metadata": {},
   "source": [
    "Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HOW PUBLICATION FREQUENCY VARIES OVER TIME \")\n",
    "\n",
    "# Calculate daily article counts\n",
    "daily_counts = df['date_only'].value_counts().sort_index()\n",
    "\n",
    "# Calculate statistics\n",
    "mean_articles = daily_counts.mean()\n",
    "std_articles = daily_counts.std()\n",
    "spike_threshold = mean_articles + std_articles\n",
    "\n",
    "print(f\"Publication Frequency Analysis:\")\n",
    "print(f\"Average articles per day: {mean_articles:.1f}\")\n",
    "print(f\"Standard deviation: {std_articles:.1f}\")\n",
    "print(f\"Spike threshold: {spike_threshold:.1f} articles (mean + 1 std)\")\n",
    "\n",
    "# Identify spike days\n",
    "spike_days = daily_counts[daily_counts > spike_threshold]\n",
    "print(f\"   • Found {len(spike_days)} spike days with unusually high volume\")\n",
    "\n",
    "print(f\"\\nDaily Article Range:\")\n",
    "print(f\"Minimum: {daily_counts.min()} articles on {daily_counts.idxmin()}\")\n",
    "print(f\"Maximum: {daily_counts.max()} articles on {daily_counts.idxmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main frequency over time plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot daily frequency\n",
    "plt.plot(daily_counts.index, daily_counts.values, \n",
    "         color='blue', alpha=0.7, linewidth=1.5, label='Daily Articles')\n",
    "\n",
    "# Add reference lines\n",
    "plt.axhline(y=mean_articles, color='red', linestyle='--', \n",
    "            label=f'Average: {mean_articles:.1f} articles')\n",
    "plt.axhline(y=spike_threshold, color='orange', linestyle='--', \n",
    "            label=f'Spike Threshold: {spike_threshold:.1f}')\n",
    "\n",
    "# Highlight spike days in red\n",
    "spike_dates = spike_days.index\n",
    "spike_values = spike_days.values\n",
    "plt.scatter(spike_dates, spike_values, color='red', s=50, zorder=5, \n",
    "            label=f'Spike Days ({len(spike_days)} days)')\n",
    "\n",
    "plt.title('How Publication Frequency Varies Over Time', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fdf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARE THERE SPIKES RELATED TO SPECIFIC MARKET EVENTS?\")\n",
    "\n",
    "print(f\"\\nAnalyzing {len(spike_days)} spike days for potential market events...\")\n",
    "\n",
    "for i, (spike_date, article_count) in enumerate(spike_days.head(8).items(), 1):\n",
    "    # Get articles from this spike day\n",
    "    spike_articles = df[df['date_only'] == spike_date]\n",
    "    \n",
    "    print(f\"\\n{i}.{spike_date} - {article_count} articles (SPIKE):\")\n",
    "    \n",
    "    # Analyze content for market events\n",
    "    all_headlines = ' '.join(spike_articles['headline'].astype(str)).lower()\n",
    "    \n",
    "    # Look for specific market-related keywords\n",
    "    market_keywords = {\n",
    "        'earnings': 'earnings',\n",
    "        'fda': 'fda|approval', \n",
    "        'merger': 'merge|acquisition|buyout',\n",
    "        'stock move': 'stock|share|trading',\n",
    "        'economic': 'economy|inflation|rate|fed',\n",
    "        'company news': 'apple|google|amazon|microsoft|tesla'\n",
    "    }\n",
    "    \n",
    "    found_events = []\n",
    "    for event_type, pattern in market_keywords.items():\n",
    "        matches = re.findall(pattern, all_headlines)\n",
    "        if matches:\n",
    "            found_events.append(f\"{event_type} ({len(matches)} mentions)\")\n",
    "    \n",
    "    # Show top keywords from spike day\n",
    "    words = re.findall(r'\\b[a-zA-Z]{4,}\\b', all_headlines)\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    common_spike_words = Counter(filtered_words).most_common(5)\n",
    "    \n",
    "    print(f\"Detected events: {', '.join(found_events) if found_events else 'General market news'}\")\n",
    "    print(f\"Top keywords: {[word for word, count in common_spike_words]}\")\n",
    "    print(f\" Sample headlines:\")\n",
    "    for headline in spike_articles['headline'].head(2):\n",
    "        print(f\"{headline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45feb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PUBLISHING TIME ANALYSIS\")\n",
    "\n",
    "# Extract hour from datetime\n",
    "df['publish_hour'] = df['datetime'].dt.hour\n",
    "\n",
    "# Count articles by hour\n",
    "hourly_counts = df['publish_hour'].value_counts().sort_index()\n",
    "\n",
    "print(\"Articles Published by Hour:\")\n",
    "for hour, count in hourly_counts.items():\n",
    "    print(f\"   {hour}:00 - {hour}:59: {count:>4} articles\")\n",
    "\n",
    "# Find peak publishing times\n",
    "peak_hour = hourly_counts.idxmax()\n",
    "peak_count = hourly_counts.max()\n",
    "\n",
    "print(f\"\\nPeak Publishing Time: {peak_hour}:00 with {peak_count} articles\")\n",
    "print(f\"   This is when {peak_count/len(df)*100:.1f}% of daily news is released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94256c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple bar chart of publishing times\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(hourly_counts.index, hourly_counts.values, color='blue', alpha=0.7)\n",
    "plt.xlabel('Hour of Day (24-hour format)')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.title('Publishing Time Distribution - When News is Released')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the peak hour\n",
    "plt.axvline(x=peak_hour, color='red', linestyle='--', alpha=0.8, \n",
    "            label=f'Peak: {peak_hour}:00 ({peak_count} articles)')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266aec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WHICH PUBLISHERS CONTRIBUTE MOST TO THE NEWS FEED?\")\n",
    "\n",
    "# Count articles per publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "print(\"Top 15 Publishers by Article Volume:\")\n",
    "top_15_publishers = publisher_counts.head(15)\n",
    "for i, (publisher, count) in enumerate(top_15_publishers.items(), 1):\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{i:2d}. {publisher:30} : {count:>4} articles ({percentage:.1f}%)\")\n",
    "\n",
    "total_top_15 = top_15_publishers.sum()\n",
    "print(f\"\\nTop 15 publishers account for {total_top_15/len(df)*100:.1f}% of all articles\")\n",
    "print(f\"Top publisher '{publisher_counts.index[0]}' contributes {publisher_counts.iloc[0]/len(df)*100:.1f}% of content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62862774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top publishers\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_10 = publisher_counts.head(10)\n",
    "plt.barh(range(len(top_10)), top_10.values, color='steelblue')\n",
    "plt.yticks(range(len(top_10)), top_10.index)\n",
    "plt.xlabel('Number of Articles')\n",
    "plt.title('Top 10 Publishers by Contribution to News Feed')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_10.values):\n",
    "    plt.text(v + 3, i, str(v), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc52b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IS THERE A DIFFERENCE IN THE TYPE OF NEWS THEY REPORT?\")\n",
    "\n",
    "# Analyze top 5 publishers' content differences\n",
    "top_5_publishers = publisher_counts.head(5).index\n",
    "\n",
    "print(\" Analyzing news type differences among top 5 publishers...\")\n",
    "\n",
    "for publisher in top_5_publishers:\n",
    "    publisher_articles = df[df['publisher'] == publisher]\n",
    "    \n",
    "    print(f\"\\n{publisher}:\")\n",
    "    print(f\"   Total articles: {len(publisher_articles)}\")\n",
    "    \n",
    "    # Analyze their focus areas\n",
    "    all_headlines = ' '.join(publisher_articles['headline'].astype(str)).lower()\n",
    "    \n",
    "    # Check for common financial topics\n",
    "    topics = {\n",
    "        'Earnings': 'earnings|profit|revenue',\n",
    "        'M&A': 'merge|acquisition|buyout',\n",
    "        'Stock Moves': 'stock|share|trading',\n",
    "        'Regulatory': 'fda|approval|regulation',\n",
    "        'Price Targets': 'price target|upgrade|downgrade',\n",
    "        'Economic': 'economy|inflation|rate'\n",
    "    }\n",
    "    \n",
    "    topic_counts = {}\n",
    "    for topic, pattern in topics.items():\n",
    "        matches = re.findall(pattern, all_headlines)\n",
    "        topic_counts[topic] = len(matches)\n",
    "    \n",
    "    # Show top 3 focus areas\n",
    "    top_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"   Top focus areas: {', '.join([f'{topic} ({count})' for topic, count in top_topics])}\")\n",
    "    \n",
    "    # Show sample headlines to illustrate their style\n",
    "    sample_headlines = publisher_articles['headline'].head(2).tolist()\n",
    "    print(f\"   Sample headlines:\")\n",
    "    for headline in sample_headlines:\n",
    "        print(f\"{headline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare focus areas across top publishers\n",
    "print(\"\\n CONTENT FOCUS COMPARISON ACROSS TOP PUBLISHERS \")\n",
    "\n",
    "# Define common topics to compare\n",
    "comparison_topics = ['Earnings', 'M&A', 'Stock Moves', 'Regulatory', 'Price Targets']\n",
    "\n",
    "topic_data = []\n",
    "for publisher in top_5_publishers:\n",
    "    publisher_articles = df[df['publisher'] == publisher]\n",
    "    all_headlines = ' '.join(publisher_articles['headline'].astype(str)).lower()\n",
    "    \n",
    "    publisher_topics = {}\n",
    "    for topic in comparison_topics:\n",
    "        pattern = {\n",
    "            'Earnings': 'earnings|profit|revenue',\n",
    "            'M&A': 'merge|acquisition|buyout',\n",
    "            'Stock Moves': 'stock|share|trading',\n",
    "            'Regulatory': 'fda|approval|regulation',\n",
    "            'Price Targets': 'price target|upgrade|downgrade'\n",
    "        }[topic]\n",
    "        \n",
    "        matches = re.findall(pattern, all_headlines)\n",
    "        # Normalize by number of articles\n",
    "        normalized_count = len(matches) / len(publisher_articles) * 100\n",
    "        publisher_topics[topic] = normalized_count\n",
    "    \n",
    "    topic_data.append(publisher_topics)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(topic_data, index=top_5_publishers)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "comparison_df.plot(kind='bar', width=0.8, figsize=(12, 6))\n",
    "plt.title('News Focus Comparison Across Top Publishers')\n",
    "plt.ylabel('Percentage of Articles Mentioning Topic (%)')\n",
    "plt.xlabel('Publisher')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Topic Focus')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60208d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EMAIL DOMAIN ANALYSIS FOR PUBLISHER NAMES\")\n",
    "\n",
    "# Check if any publishers are email addresses\n",
    "email_mask = df['publisher'].str.contains(r'@', na=False)\n",
    "email_publishers = df[email_mask]\n",
    "\n",
    "print(f\"Found {len(email_publishers)} articles with email addresses as publisher names\")\n",
    "print(f\"This represents {len(email_publishers)/len(df)*100:.1f}% of all articles\")\n",
    "\n",
    "if len(email_publishers) > 0:\n",
    "    # Extract domains from email addresses\n",
    "    email_publishers = email_publishers.copy()\n",
    "    email_publishers['domain'] = email_publishers['publisher'].str.extract(r'@([\\w\\.-]+)')\n",
    "    \n",
    "    # Count articles by domain\n",
    "    domain_counts = email_publishers['domain'].value_counts()\n",
    "    \n",
    "    print(f\"\\nArticles by Email Domain:\")\n",
    "    for domain, count in domain_counts.head(10).items():\n",
    "        percentage = (count / len(email_publishers)) * 100\n",
    "        print(f\"   {domain:25} : {count:>3} articles ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualize top domains\n",
    "    if len(domain_counts) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        top_domains = domain_counts.head(8)\n",
    "        plt.barh(range(len(top_domains)), top_domains.values, color='purple', alpha=0.7)\n",
    "        plt.yticks(range(len(top_domains)), top_domains.index)\n",
    "        plt.xlabel('Number of Articles')\n",
    "        plt.title('Top Email Domains in Publisher Names')\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(top_domains.values):\n",
    "            plt.text(v + 0.5, i, str(v), va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTop domain '{domain_counts.index[0]}' contributes {domain_counts.iloc[0]} articles\")\n",
    "        print(f\" Top 3 domains account for {domain_counts.head(3).sum()/len(email_publishers)*100:.1f}% of email-published articles\")\n",
    "    \n",
    "else:\n",
    "    print(\"No email addresses found in publisher names - using regular organization names instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a78f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"ORGANIZATION ANALYSIS - UNIQUE PUBLISHER DOMAINS\")\n",
    "\n",
    "# Count articles by publisher (this works regardless of email situation)\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "# If no emails, analyze the regular publisher names as organizations\n",
    "if len(email_publishers) == 0:\n",
    "    print(\"Analyzing regular publisher names as organizations...\")\n",
    "    \n",
    "    print(f\"\\nTop Organizations by Article Volume:\")\n",
    "    top_orgs = publisher_counts.head(10)\n",
    "    for i, (org, count) in enumerate(top_orgs.items(), 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{i:2d}. {org:30} : {count:>4} articles ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Calculate concentration\n",
    "    top_5_orgs = publisher_counts.head(5)\n",
    "    top_10_orgs = publisher_counts.head(10)\n",
    "    \n",
    "    print(f\"\\nOrganization Concentration:\")\n",
    "    print(f\"   • Top 5 organizations: {top_5_orgs.sum()/len(df)*100:.1f}% of articles\")\n",
    "    print(f\"   • Top 10 organizations: {top_10_orgs.sum()/len(df)*100:.1f}% of articles\")\n",
    "    print(f\"   • Remaining {len(publisher_counts) - 10} organizations: {publisher_counts[10:].sum()/len(df)*100:.1f}% of articles\")\n",
    "\n",
    "else:\n",
    "    print(\"Email addresses found - showing combined analysis:\")\n",
    "    print(f\"Top regular publisher: '{publisher_counts.index[0]}' with {publisher_counts.iloc[0]} articles\")\n",
    "\n",
    "print(f\"\\nInsight: News feed is dominated by a small number of organizations\")\n",
    "print(f\"   The top publisher accounts for {publisher_counts.iloc[0]/len(df)*100:.1f}% of all content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e452ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
